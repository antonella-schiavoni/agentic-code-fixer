"""Core data types and structures for the Agentic Code Fixer system.

This module defines the fundamental data models used throughout the system for
representing patches, evaluation results, experiment metadata, and other core
entities. All models use Pydantic for validation and serialization.

The module provides type-safe representations for:
- Patch candidates generated by AI agents
- Evaluation results from patch comparison systems
- Experiment metadata and tracking information
- Agent configurations and code context
- Test results and ELO ratings

These types form the backbone of data flow between different components
of the automated patch generation and evaluation pipeline.
"""

from __future__ import annotations

import uuid
from datetime import datetime
from enum import Enum
from typing import Any

from pydantic import BaseModel, Field


class PatchStatus(str, Enum):
    """Enumeration of possible states for a patch candidate throughout its lifecycle.

    This enum tracks the progression of a patch from initial generation through
    evaluation, application, and testing phases. Each status represents a distinct
    stage in the automated patch processing pipeline.

    Attributes:
        GENERATED: Patch has been created by an AI agent but not yet evaluated.
        EVALUATED: Patch has undergone comparison evaluation against other candidates.
        APPLIED: Patch has been successfully applied to the target codebase.
        TESTED: Patch has been applied and automated tests have been executed.
        FAILED: Patch failed at some stage (evaluation, application, or testing).
    """

    GENERATED = "generated"
    EVALUATED = "evaluated"
    APPLIED = "applied"
    TESTED = "tested"
    FAILED = "failed"


class EvaluationMethod(str, Enum):
    """Available methods for evaluating and ranking patch candidates.

    The system uses ELO tournament rating as the primary evaluation method.
    This provides robust ranking for multiple patch candidates using a chess-style
    rating system where patches gain/lose points based on pairwise comparison outcomes.

    Attributes:
        ELO_TOURNAMENT: Chess-style rating system where patches gain/lose points
            based on comparison outcomes. Converges to stable rankings over multiple
            rounds and works well with large numbers of candidates.
    """

    ELO_TOURNAMENT = "elo_tournament"


class PatchCandidate(BaseModel):
    """A patch candidate generated by an AI agent to fix a specific code issue.

    This class represents a proposed code change that could potentially solve
    a reported bug or issue. Each patch contains the actual code modification,
    metadata about where it should be applied, and tracking information about
    its source and evaluation status.

    The patch content represents a complete replacement for the specified line
    range in the target file. Line numbers are zero-indexed to match standard
    editor conventions.

    Attributes:
        id: Unique identifier for this patch candidate.
        content: The actual code content that will replace the existing lines.
        description: Human-readable explanation of what this patch accomplishes.
        agent_id: Identifier of the AI agent that generated this patch.
        file_path: Relative path to the target file within the repository.
        line_start: Zero-indexed starting line number for the replacement.
        line_end: Zero-indexed ending line number for the replacement (inclusive).
        confidence_score: Agent's confidence in this solution (0.0 to 1.0).
        status: Current processing status of this patch.
        created_at: Timestamp when this patch was generated.
        metadata: Additional key-value data about the patch generation process.
    """

    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    content: str = Field(description="The actual patch content")
    description: str = Field(description="Human-readable description of the patch")
    agent_id: str = Field(description="ID of the agent that generated this patch")
    file_path: str = Field(description="Target file path for the patch")
    line_start: int = Field(description="Starting line number")
    line_end: int = Field(description="Ending line number")
    confidence_score: float = Field(ge=0.0, le=1.0, description="Agent confidence")
    status: PatchStatus = Field(default=PatchStatus.GENERATED)
    created_at: datetime = Field(default_factory=datetime.now)
    metadata: dict[str, Any] = Field(default_factory=dict)


class EvaluationResult(BaseModel):
    """Result of a pairwise comparison between two patch candidates.

    This class captures the outcome when an evaluator (typically an LLM) compares
    two patch candidates and determines which one is superior. The evaluation
    includes both the decision and the reasoning behind it.

    These results are used to build win/loss records for patches in AB testing
    scenarios or to update ELO ratings in tournament-style evaluations.

    Attributes:
        patch_a_id: Unique identifier of the first patch in the comparison.
        patch_b_id: Unique identifier of the second patch in the comparison.
        winner_id: Unique identifier of the patch deemed superior (must match
            either patch_a_id or patch_b_id).
        confidence: Evaluator's confidence in the decision (0.0 to 1.0).
        reasoning: Detailed explanation of why the winner was chosen.
        evaluation_time: Timestamp when this evaluation was performed.
        metadata: Additional information about the evaluation process.
    """

    patch_a_id: str
    patch_b_id: str
    winner_id: str
    confidence: float = Field(ge=0.0, le=1.0)
    reasoning: str
    evaluation_time: datetime = Field(default_factory=datetime.now)
    metadata: dict[str, Any] = Field(default_factory=dict)


class EloRating(BaseModel):
    """ELO rating statistics for a patch candidate in tournament evaluation.

    This class tracks the chess-style ELO rating for a patch, which evolves
    based on wins and losses against other patches. Higher ratings indicate
    patches that consistently outperform others in head-to-head comparisons.

    The ELO system allows patches to be ranked even when they haven't been
    directly compared to every other patch, making it efficient for large
    numbers of candidates.

    Attributes:
        patch_id: Unique identifier of the patch being rated.
        rating: Current ELO rating (starts at 1200, standard chess default).
        matches_played: Total number of pairwise comparisons this patch has
            participated in.
        wins: Number of comparisons where this patch was deemed superior.
        losses: Number of comparisons where this patch was deemed inferior.
        last_updated: Timestamp of the most recent rating update.
    """

    patch_id: str
    rating: float = Field(default=1200.0)
    matches_played: int = Field(default=0)
    wins: int = Field(default=0)
    losses: int = Field(default=0)
    last_updated: datetime = Field(default_factory=datetime.now)


class ExperimentMetadata(BaseModel):
    """Comprehensive metadata for a complete patch generation experiment.

    This class serves as the central record for an entire automated patch
    generation run, from initial configuration through final results. It
    captures both the experimental setup and the outcomes, enabling detailed
    analysis and reproducibility of results.

    An experiment represents one complete cycle of: problem analysis, patch
    generation by multiple agents, evaluation of candidates, selection of
    the best patch, application to the codebase, and validation through testing.

    Attributes:
        experiment_id: Unique identifier for this experimental run.
        repository_path: Absolute path to the target code repository.
        problem_description: Human description of the issue being addressed.
        num_agents: Number of AI agents used for patch generation.
        num_patches_generated: Total number of patch candidates created.
        evaluation_method: Method used to compare and rank patches.
        winning_patch_id: Identifier of the patch selected as best, if any.
        start_time: When the experiment began execution.
        end_time: When the experiment completed (successfully or not).
        total_duration_seconds: Total runtime of the experiment.
        success: Whether the experiment completed without errors.
        error_message: Details of any failure that prevented completion.
        test_results: Summary of test execution results for the winning patch.
    """

    experiment_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    repository_path: str
    problem_description: str
    num_agents: int
    num_patches_generated: int
    evaluation_method: EvaluationMethod
    winning_patch_id: str | None = None
    start_time: datetime = Field(default_factory=datetime.now)
    end_time: datetime | None = None
    total_duration_seconds: float | None = None
    success: bool = False
    error_message: str | None = None
    test_results: dict[str, Any] = Field(default_factory=dict)


class CodeContext(BaseModel):
    """Code context information used to inform patch generation decisions.

    This class represents a chunk of source code along with metadata that
    helps AI agents understand the codebase structure and dependencies.
    Code contexts are created during the indexing phase and retrieved
    based on semantic similarity to the problem description.

    The embedding vector enables efficient similarity search, while the
    extracted functions and dependencies provide structured information
    about the code's purpose and relationships.

    Attributes:
        file_path: Relative path to the source file within the repository.
        content: The actual source code content for this context chunk.
        language: Programming language (e.g., 'python', 'javascript').
        embedding: Vector representation for semantic similarity search.
        relevant_functions: Function and class names defined in this context.
        dependencies: Import statements and external dependencies referenced.
    """

    file_path: str
    content: str
    language: str
    embedding: list[float] | None = None
    relevant_functions: list[str] = Field(default_factory=list)
    dependencies: list[str] = Field(default_factory=list)


class AgentConfig(BaseModel):
    """Configuration parameters for an individual AI agent.

    This class defines how a specific AI agent should behave when generating
    patch candidates. Different agents can have different specializations,
    model parameters, and prompting strategies to encourage diverse solutions.

    The system supports multiple agents working in parallel, each with their
    own configuration, to increase the variety and quality of generated patches.

    Attributes:
        agent_id: Unique identifier for this agent configuration.
        model_name: Name of the language model to use (e.g., 'claude-3-5-sonnet').
        temperature: Sampling temperature for response generation (0.0 to 1.0).
            Lower values produce more deterministic outputs.
        max_tokens: Maximum number of tokens in the agent's response.
        system_prompt: Base instructions that define the agent's behavior.
        specialized_role: Optional role specialization (e.g., 'security',
            'performance', 'general') that influences prompting strategy.
    """

    agent_id: str
    model_name: str
    temperature: float = Field(default=0.7, ge=0.0, le=1.0)
    max_tokens: int = Field(default=2048, gt=0)
    system_prompt: str = ""
    specialized_role: str | None = None


class TestResult(BaseModel):
    """Comprehensive results from executing tests after applying a patch.

    This class captures the complete outcome of running automated tests
    on a codebase after a patch has been applied. It includes both the
    execution details and analysis of any test failures.

    The test results are crucial for validating that a patch actually
    fixes the intended issue without introducing regressions or new bugs.

    Attributes:
        test_command: The exact command executed to run the tests.
        exit_code: Process exit code (0 typically indicates success).
        stdout: Standard output from the test execution.
        stderr: Standard error output from the test execution.
        duration_seconds: Total time taken to execute all tests.
        passed: Whether all tests passed (exit_code == 0 and no failures).
        failed_tests: List of specific test names that failed.
        new_failures: Tests that failed after the patch but passed in baseline.
            This helps identify regressions introduced by the patch.
        timestamp: When the test execution completed.
    """

    test_command: str
    exit_code: int
    stdout: str
    stderr: str
    duration_seconds: float
    passed: bool
    failed_tests: list[str] = Field(default_factory=list)
    new_failures: list[str] = Field(default_factory=list)
    timestamp: datetime = Field(default_factory=datetime.now)